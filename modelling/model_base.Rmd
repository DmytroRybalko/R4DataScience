---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```
Lets take a look at the simulated dataset sim1
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```
Let's add some noise:
```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_point(colour = "red") +
  geom_point(data = models, aes(a1, a2))
```
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```
Turn our model family into an R function
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
```
Next, we need some way to compute an overall distance between the predicted and actual values
```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```
Define a helper function
```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}
```
Use purrr to compute the distance for all the models defined above
```{r}
models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```
Next, letâ€™s overlay the 10 best models on to the data
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```
Another way
```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```
Generate an evenly spaced grid of points (this is called a grid search)
```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
```
```{r}
grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```
Find best model with optim():
```{r}
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
```
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```
Make the same things with lm()
```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```
#Exercises
##1
```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
sim1a_mod <- lm(y ~ x, data = sim1a)
```
```{r}
ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = coef(sim1a_mod)[[1]], slope = coef(sim1a_mod)[[2]])
```
##Visualization
```{r}
grid <- sim1 %>% 
  data_grid(x) 
grid
```
add_predictor()
```{r}

df <- tibble::data_frame(
x = sort(runif(100)),
y = 5 * x + 0.5 * x ^ 2 + 3 + rnorm(length(x))
)
plot(df)
m1 <- lm(y ~ x, data = df)
tgrid <- data.frame(x = seq(0, 1, length = 10))
tgrid %>% add_predictions(m1)
```
spread_predictions:
```{r}
m2 <- lm(y ~ poly(x, 2), data = df)
tgrid %>% spread_predictions(m1, m2)
tgrid %>% gather_predictions(m1, m2)
```
### Next we add predictions
```{r}
grid <- grid %>% 
  add_predictions(sim1_mod) 
grid
```
### Next, we plot the predictions.
```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)
```
##Residuals
```{r}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1
```
Draw a frequency polygon to help us understand the spread of the residuals:
```{r}
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```
## Formulas and model families
```{r}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
model_matrix(df, y ~ x2)
```
### Examples for understanding
```{r}
model_matrix(mtcars, mpg ~ cyl) %>% 
  mutate(MPG = mtcars$mpg, CYL = mtcars$cyl)
```
### Categorical variables
```{r}
df11 <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 1,
  "male", 1
)
model_matrix(df11, response ~ sex)
```
```{r}
sim3
```

